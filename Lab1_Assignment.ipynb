{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Lab1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1bls46SNnk0"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "Lab overview:\n",
        "\n",
        "\n",
        "* Normalization\n",
        "* Tokenization\n",
        "* Lematization\n",
        "* Stemming\n",
        "* Stopwords removal\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nK8jcjSNkv3"
      },
      "source": [
        "##Text normalization (cleaning)\n",
        "\n",
        "Depending on the task you are cleaning the text for, you may perform one or more of: \n",
        "\n",
        "* Transform text to lowercase\n",
        "* Remove emoticons ( :) :D) and emojis (üíô üê±)\n",
        "* Remove punctuation\n",
        "* Remove digits or transform them to words\n",
        "* Correct spelling errors\n",
        "\n",
        "\n",
        "Python Regular Expressions \n",
        "*   [re Python documentation](https://docs.python.org/3/library/re.html)\n",
        "*   [Quick reference](https://www.computerhope.com/unix/regex-quickref.htm)\n",
        "*   [Cheat Sheet](https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf)\n",
        "\n",
        "![regular_expressions](https://res.cloudinary.com/practicaldev/image/fetch/s--_iE0KvdT--/c_imagga_scale,f_auto,fl_progressive,h_900,q_auto,w_1600/https://dev-to-uploads.s3.amazonaws.com/i/zpek00ubevoxvn458b01.png)\n",
        "\n",
        "[Photo source](https://dev.to/mconner89/regular-expressions-grouping-and-string-methods-3ijn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChzKaNrWZ16E"
      },
      "source": [
        "Here is our text sample, a short review of the movie [Jaws](https://en.wikipedia.org/wiki/Jaws_(film))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwbmESJGWAre",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0118ac9f-b6df-44d9-a628-af59d338d5e2"
      },
      "source": [
        "text = '\" Jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. The movie opens with blackness, and only distant, alien-like underwater sounds. :) :D It deserves 5 stars, not 4 stars.'\n",
        "text"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" Jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. The movie opens with blackness, and only distant, alien-like underwater sounds. :) :D It deserves 5 stars, not 4 stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV-zwIaLJ_gI"
      },
      "source": [
        "Transform text to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEhAwxLyaX7L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3f19ae1d-5643-42e8-e616-eeb72d627558"
      },
      "source": [
        "text = text.lower()\n",
        "text"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves 5 stars, not 4 stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htuAG8zKarGo"
      },
      "source": [
        "importing [re](https://docs.python.org/3/library/re.html) library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOeMPOhga09l"
      },
      "source": [
        "import re"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4ioMoqMP4sW"
      },
      "source": [
        "Remove digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PM0guG_P60M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fa0d9e66-3cef-4e95-f562-0aab67aba0b4"
      },
      "source": [
        "re.sub(' \\d+', '', text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves stars, not stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeQEFmcfRPYc"
      },
      "source": [
        "Converting numbers to words using [num2words](https://github.com/savoirfairelinux/num2words) (it works on multiple languages)\n",
        "\n",
        "We need to install the num2words library first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vx3KhgnRPfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ce0909-5e37-438d-d7cc-f7e7e3cc2c7c"
      },
      "source": [
        "!pip install num2words"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñà‚ñà‚ñé                            | 10 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 20 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 30 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 40 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 51 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 61 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 71 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 81 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 92 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words) (0.6.2)\n",
            "Installing collected packages: num2words\n",
            "Successfully installed num2words-0.5.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1K45N29RlOX"
      },
      "source": [
        "After installing, we can import it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Egr4m_9Rl9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "26570b7d-a5c5-4001-a628-6dc1816dd8b1"
      },
      "source": [
        "from num2words import num2words\n",
        "\n",
        "text = ' '.join([num2words(word) if word.isdigit() else word for word in text.split()])\n",
        "text\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves five stars, not four stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbEeAr6pKFj0"
      },
      "source": [
        "Remove emoticons ( :) :D) and emojis (üíô üê±)\n",
        "\n",
        "Using [emoji](https://github.com/carpedm20/emoji) library or the corresponding unicode characters.\n",
        "\n",
        "We need to install the emoji library first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq72wdvEKI7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad126d9-9323-4d38-ef21-cc190ffef8ba"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñà                              | 10 kB 27.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñâ                            | 20 kB 33.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 30 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 40 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 51 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 61 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 71 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 81 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 92 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 102 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 112 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 122 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 133 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 143 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 153 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 163 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170 kB 9.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=ff4b39f4b27c73e228c1aa11a8fbff96834a6948dddc43401929bdedc9441bc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVu11QZEKoSW"
      },
      "source": [
        "After installing, we can import it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HYmbrutKnEo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0779b946-4922-4703-938c-8f966411c439"
      },
      "source": [
        "import emoji\n",
        "\n",
        "emoji.get_emoji_regexp().sub(u'', text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" jaws \"  is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves five stars, not four stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N69dNffKLtqu"
      },
      "source": [
        "The *get_emoji_regexp()* function returns a regex to match any emoji.\n",
        "\n",
        "Another way of removing emojis with regex:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHSh8NUIM_bY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e2d9a29b-5470-49b9-8c60-364ffe225d3f"
      },
      "source": [
        "emoj = re.compile(\"[\"\n",
        "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "    u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "    u\"\\U00002702-\\U000027B0\"\n",
        "    u\"\\U00002702-\\U000027B0\"\n",
        "    u\"\\U000024C2-\\U0001F251\"\n",
        "    u\"\\U0001f926-\\U0001f937\"\n",
        "    u\"\\U00010000-\\U0010ffff\"\n",
        "    u\"\\u2640-\\u2642\" \n",
        "    u\"\\u2600-\\u2B55\"\n",
        "    u\"\\u200d\"\n",
        "    u\"\\u23cf\"\n",
        "    u\"\\u23e9\"\n",
        "    u\"\\u231a\"\n",
        "    u\"\\ufe0f\"\n",
        "    u\"\\u3030\"\n",
        "    \"]+\", re.UNICODE)\n",
        "\n",
        "text = re.sub(emoj, '', text)\n",
        "text"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" jaws \"  is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds. :) :d it deserves five stars, not four stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_xacsPCOPjL"
      },
      "source": [
        "Removing emoticons (regex from [nltk Twitter Tokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/casual.py))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeYkCRPNOS3n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d26592d3-5727-4a00-f8b5-0d9207890cf7"
      },
      "source": [
        "emoticon_string = r\"\"\"\n",
        "    (?:\n",
        "      [<>]?\n",
        "      [:;=8]                     # eyes\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      |\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [:;=8]                     # eyes\n",
        "      [<>]?\n",
        "      |\n",
        "      </?3                       # heart\n",
        "    )\"\"\"\n",
        "    \n",
        "emoticon_re = re.compile(emoticon_string, re.VERBOSE | re.I | re.UNICODE)\n",
        "text = re.sub(emoticon_re, '', text)\n",
        "text"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen. the movie opens with blackness, and only distant, alien-like underwater sounds.   it deserves five stars, not four stars.'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VkbsLqXMyYo"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "\n",
        "*   Word level: Split by whitespace, [nltk.word_tokenize](https://www.nltk.org/api/nltk.tokenize.html)\n",
        "*   Sentence level: Split by punctuation, [nltk.sent_tokenize](https://www.nltk.org/api/nltk.tokenize.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrN047ULHZQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b97d4a3-0087-4024-8db3-87c7eec88550"
      },
      "source": [
        "print(text.split())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"', 'jaws', '\"', 'ü¶àü¶àü¶à', 'is', 'a', 'rare', 'film', 'that', 'grabs', 'your', 'attention', 'before', 'it', 'shows', 'you', 'a', 'single', 'image', 'on', 'screen.', 'the', 'movie', 'opens', 'with', 'blackness,', 'and', 'only', 'distant,', 'alien-like', 'underwater', 'sounds.', 'it', 'deserves', 'five', 'stars,', 'not', 'four', 'stars.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhyTVP55VRwT"
      },
      "source": [
        "We need to download first the Punkt Tokenizer Models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHvjfBzxVdAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412cc3e4-07d2-4b0c-e946-721f52e5ac80"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nZasrqFVE-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241bd3d7-d7ca-473c-93a1-a2edf488707c"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "tokenized_text_nltk = word_tokenize(text)\n",
        "print(tokenized_text_nltk)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['``', 'jaws', '``', 'ü¶àü¶àü¶à', 'is', 'a', 'rare', 'film', 'that', 'grabs', 'your', 'attention', 'before', 'it', 'shows', 'you', 'a', 'single', 'image', 'on', 'screen', '.', 'the', 'movie', 'opens', 'with', 'blackness', ',', 'and', 'only', 'distant', ',', 'alien-like', 'underwater', 'sounds', '.', 'it', 'deserves', 'five', 'stars', ',', 'not', 'four', 'stars', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yypjBlMVpnG"
      },
      "source": [
        "Sentence tokenization using regex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqqArIwwVp1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b04d0f-2ad2-4049-ae74-9c17ffacb1d5"
      },
      "source": [
        " re.split('(?<=[.!?]) +', text)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen.',\n",
              " 'the movie opens with blackness, and only distant, alien-like underwater sounds.',\n",
              " 'it deserves five stars, not four stars.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7KKoyA1WNVo"
      },
      "source": [
        "Sentence tokenization using nltk.sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3RfpgBzWSaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee145ad-4bf3-49e6-8d85-a2c355941493"
      },
      "source": [
        "nltk.sent_tokenize(text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\" jaws \" ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen.',\n",
              " 'the movie opens with blackness, and only distant, alien-like underwater sounds.',\n",
              " 'it deserves five stars, not four stars.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM9idFgSWnJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1de702-ea58-4a69-9ee4-73e6b1d0a993"
      },
      "source": [
        "text_example = 'I was good.Thanks.'\n",
        "re.split('(?<=[.!?]) +', text_example)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I was good.Thanks.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0DKB0VoWr8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "934c5c35-872b-4815-b97a-2b5079352fa7"
      },
      "source": [
        "nltk.sent_tokenize(text_example)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I was good.Thanks.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1s4GXfHXCOl"
      },
      "source": [
        "Removing punctuation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCxHclC0XF8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7758ede1-3dab-47d4-df21-587589435ca7"
      },
      "source": [
        "re.sub(r'[^\\w\\s]','', text)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' jaws   is a rare film that grabs your attention before it shows you a single image on screen the movie opens with blackness and only distant alienlike underwater sounds   it deserves five stars not four stars'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYxNlaSgi6gH"
      },
      "source": [
        "Using [string](https://docs.python.org/3/library/string.html) library. \n",
        "\n",
        "The string.punctuation method returns a list of punctuation marks. \n",
        "\n",
        "We use the translate() method which replaces every instance of a punctuation mark with the value '' in our strings. We use the str.maketrans() method to support the translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jid14LagYAPQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3976be02-7744-4082-a678-2508487c6979"
      },
      "source": [
        "import string\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "text"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' jaws  ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen the movie opens with blackness and only distant alienlike underwater sounds   it deserves five stars not four stars'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOkRYWmakLla"
      },
      "source": [
        "Removing multiple spaces between words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHAFnlSzYu7_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b2ba90b1-77f0-4ef5-c0d4-596adfb80e43"
      },
      "source": [
        "text = re.sub(' +', ' ', text)\n",
        "text"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' jaws ü¶àü¶àü¶à is a rare film that grabs your attention before it shows you a single image on screen the movie opens with blackness and only distant alienlike underwater sounds it deserves five stars not four stars'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F0pEMhIHbd6"
      },
      "source": [
        "## Removing stopwords\n",
        "\n",
        "![stopwords.jpg](https://user.oc-static.com/upload/2021/01/06/16099626487943_P1C2.png) \n",
        "\n",
        "[Photo source](https://openclassrooms.com/en/courses/6532301-introduction-to-natural-language-processing/6980726-remove-stop-words-from-a-block-of-text)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E53_oLK7NP8y"
      },
      "source": [
        "###Why do we Need to Remove Stopwords?\n",
        "\n",
        "For tasks such as text classification, we may want to remove any unnecessary words and keep only words with meaning. \n",
        "\n",
        "Stopwords removal is not used in tasks such as machine translation or text summarization.\n",
        "\n",
        "Using [nltk](https://www.nltk.org/index.html) and [spaCy](https://spacy.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGkYhh5oMcx1"
      },
      "source": [
        "Stopwords removal using nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idPo-mzrC2HW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4259ad50-c289-449b-a749-1bd6c2a4f995"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "print(len(stop_words_nltk))\n",
        "print(stop_words_nltk)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "179\n",
            "{'yours', 'again', \"isn't\", 'but', 'most', 'aren', \"she's\", 'were', 'on', 'theirs', 'against', 'now', 'won', 'is', \"shan't\", 'nor', 'those', 'her', 'then', 'some', 'd', 'hadn', 'while', 'these', 'a', 'under', 'be', 'any', 'which', 'no', 've', 'weren', 'other', 'should', \"aren't\", 'don', 'hers', \"wouldn't\", 'over', 'having', \"weren't\", 'doing', 'up', 'into', 'had', 'such', 'haven', 'each', 'off', \"shouldn't\", 'themselves', 'who', 'our', 'she', 'or', 'once', 'how', 'with', 'out', 'they', 'from', \"mustn't\", 'm', 'the', \"you'd\", 't', 'doesn', \"wasn't\", \"couldn't\", 'll', 'yourselves', 'where', 'between', 'hasn', 'whom', 'before', 'its', 'been', 'for', \"should've\", \"you'll\", \"you're\", 'what', 'him', 'his', \"didn't\", 'more', 'very', 'we', 'do', 'yourself', 'all', 'this', 'when', 'there', 'below', \"hasn't\", 'an', 'being', 'isn', 'didn', 'can', 'myself', 'as', \"it's\", 'did', 'has', \"mightn't\", 'i', 'are', 'to', 'mightn', 'at', 'your', 'own', \"that'll\", 'of', \"hadn't\", 'because', 'ain', \"doesn't\", 'ourselves', 're', 'have', 'both', \"don't\", 'than', \"haven't\", 'ours', 'does', 'am', 'couldn', 'shouldn', 'above', 'down', 'my', 'that', 'if', 'in', 's', \"you've\", 'he', 'himself', 'about', 'me', 'and', 'further', 'ma', 'too', 'just', 'needn', 'you', \"won't\", 'shan', 'will', 'by', 'y', 'wasn', 'herself', 'why', 'not', 'wouldn', 'few', 'here', 'was', 'until', 'so', \"needn't\", 'mustn', 'after', 'their', 'it', 'during', 'same', 'through', 'only', 'itself', 'them', 'o'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY7q0410L7t0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9ea007-a8d8-459b-f410-9326597d7869"
      },
      "source": [
        "tokenized_text_without_stopwords = [i for i in tokenized_text_nltk if not i in stop_words_nltk]\n",
        "print(tokenized_text_without_stopwords)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['``', 'jaws', '``', 'ü¶àü¶àü¶à', 'rare', 'film', 'grabs', 'attention', 'shows', 'single', 'image', 'screen', '.', 'movie', 'opens', 'blackness', ',', 'distant', ',', 'alien-like', 'underwater', 'sounds', '.', 'deserves', 'five', 'stars', ',', 'four', 'stars', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux3FIzVTMYc7"
      },
      "source": [
        "Stopwords removal using spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys1CTPNoMgNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8bb9a2e-5fdf-47f8-fbdb-06c74894ce6a"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "stop_words_spacy = nlp.Defaults.stop_words\n",
        "print(len(stop_words_spacy))\n",
        "print(stop_words_spacy)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326\n",
            "{'most', 'on', 'thru', 'against', 'even', 'next', 'then', 'these', 'any', 'no', 'hence', 'whether', 'along', \"'d\", 'wherein', 'everyone', 'namely', 'others', 'somehow', 'up', 'around', 'although', 'such', 'bottom', 'who', 'our', 'with', 'out', 'they', 'from', 'using', 'much', 'anyway', 'since', 'nevertheless', 'various', 'former', 'beforehand', 'anywhere', \"'m\", 'before', 'third', 'for', 'we', 'all', 'becoming', 'serious', 'towards', '‚Äôs', 'below', 'except', 'being', 'can', 'get', 'i', 'at', 'three', 'of', 'whereupon', 'alone', 'rather', 'than', 'still', 'cannot', 'in', 'my', 'none', '‚Äòs', 'without', 'me', 'whoever', 'noone', 'enough', '‚Äôll', 'least', 'onto', 'until', 'eight', 'show', 'same', 'per', '‚Äòll', 'though', 'make', 'again', 'everything', 'top', 'were', 'five', 'twelve', 'thence', 'nor', 'together', 'some', 'beside', 'seem', 'be', 'unless', 'many', 'must', 'whereby', 'anyone', 'used', '‚Äòve', 'anyhow', 'hers', 'call', 'empty', 'however', 'had', 'nobody', '‚Äòm', 'say', 'or', 'once', 'nothing', 'sometimes', \"'re\", 'mine', 'otherwise', '‚Äôve', 'yourselves', 'sometime', 'one', \"'s\", 'more', 'several', 'besides', 'front', 'anything', 'myself', 'as', 'might', 'neither', 'seeming', 'because', 'whereafter', 're', 'have', '‚Äôre', 'behind', 'see', 'never', 'latter', 'if', 'whereas', 'within', 'about', 'further', 'just', 'hereafter', 'you', 'take', 'well', 'often', 'name', 'became', 'please', 'through', 'almost', \"n't\", 'itself', 'wherever', 'now', 'is', 'those', \"'ll\", 'thereupon', 'while', 'a', 'under', 'ca', 'thereby', 'other', 'seemed', 'toward', 'every', 'over', 'another', 'else', 'mostly', 'off', 'quite', 'whole', 'how', 'the', 'also', 'upon', '‚Äôd', 'whom', 'somewhere', 'throughout', 'him', 'yourself', 'do', 'this', 'when', 'go', 'an', 'full', 'did', 'has', 'are', 'your', 'own', '‚Äòre', 'us', 'ourselves', 'become', 'everywhere', 'done', 'first', 'something', 'forty', 'six', 'someone', 'made', 'n‚Äòt', 'back', 'ten', 'too', 'hereby', 'amongst', 'part', 'put', 'afterwards', 'why', 'few', 'fifteen', 'was', 'so', 'after', 'whatever', 'only', 'them', 'yours', 'whose', '‚Äôm', 'latterly', 'keep', 'but', 'regarding', 'elsewhere', 'due', 'four', 'beyond', 'nine', 'her', 'move', 'perhaps', 'could', 'which', 'seems', 'sixty', 'fifty', 'amount', 'give', 'should', 'already', 'doing', '‚Äòd', 'into', 'herein', 'thereafter', 'each', 'among', 'themselves', 'n‚Äôt', 'she', 'less', 'therefore', 'two', 'yet', 'where', 'between', 'been', 'formerly', 'its', \"'ve\", 'what', 'his', 'nowhere', 'very', 'twenty', 'meanwhile', 'there', 'either', 'moreover', 'hundred', 'really', 'eleven', 'to', 'whence', 'via', 'therein', 'hereupon', 'both', 'side', 'whenever', 'ours', 'does', 'am', 'above', 'down', 'that', 'may', 'whither', 'he', 'himself', 'and', 'would', 'thus', 'will', 'by', 'ever', 'herself', 'not', 'across', 'indeed', 'here', 'becomes', 'always', 'their', 'during', 'it', 'last'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE9rct__Oi7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472cdefa-e6c9-45f5-ad71-f1b879308ded"
      },
      "source": [
        "tokenized_text_spacy = nlp(text)\n",
        "tokenized_text_without_stopwords = [i for i in tokenized_text_spacy if not i in stop_words_spacy]\n",
        "print(tokenized_text_without_stopwords)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ , jaws, ü¶à, ü¶à, ü¶à, is, a, rare, film, that, grabs, your, attention, before, it, shows, you, a, single, image, on, screen, the, movie, opens, with, blackness, and, only, distant, alienlike, underwater, sounds, it, deserves, five, stars, not, four, stars]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH15qHZ3HYga"
      },
      "source": [
        "## Lematization/Stemming\n",
        "\n",
        "![1_HLQgkMt5-g5WO5VpNuTl_g.jpeg](https://miro.medium.com/max/564/1*HLQgkMt5-g5WO5VpNuTl_g.jpeg)\n",
        "\n",
        "[Photo source](https://tr.pinterest.com/pin/706854104005417976/)\n",
        "\n",
        "Using [nltk](https://www.nltk.org/index.html) and [spaCy](https://spacy.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOVeBOOYZo7F"
      },
      "source": [
        "Lematization\n",
        "\n",
        "Using the WordNetLemmatizer from nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E89_jM6NY_nh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5fb0ede-a5ec-4477-f7a9-603907f65b41"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxu3jUM1UviR"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = word_tokenize(text)\n",
        "for word in words:\n",
        "    print(word, lemmatizer.lemmatize(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWPz08R0qD79"
      },
      "source": [
        "Using the [lemmatizer](https://spacy.io/api/lemmatizer) from spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5oA4G1muu4y"
      },
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11YnUMJpqEO3"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser, etc.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "  print(token, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMUayDqZweHR"
      },
      "source": [
        "Stemming in using nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN7dDNx1weU3"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "for word in words:\n",
        "    print(word, ps.stem(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmgsTVhIZuS9"
      },
      "source": [
        "[Other stemmers in nltk](https://www.nltk.org/api/nltk.stem.html)\n",
        "\n",
        "The spacy library does not perform stemming, only lemmatization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgozL531Z2Ju"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "To be uploaded here: https://forms.gle/ygCNwFM4i5RMPtsC6\n",
        "\n",
        "Preprocess texts from Twitter\n",
        "\n",
        "## Data\n",
        "\n",
        "We will use the twitter corpus from nltk, usually used in sentiment analysis.\n",
        "\n",
        "The fist step is downloading the dataset using the *download* function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fLAUv1MRdHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4085aa0-8535-493a-feee-d9331424c615"
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68_VdfEmjKXh"
      },
      "source": [
        "In order to inspect our data, we look at the first 25 tweets from the dataset. The text contains a lot of mentions, hashtags and emoticons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIODYYSvSuWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fec511c-e99c-46a0-fec9-7b24f95aee8b"
      },
      "source": [
        "from nltk.corpus  import twitter_samples\n",
        "\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "tweets = tweets[:25]\n",
        "tweets"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
              " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
              " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n",
              " '@97sides CONGRATS :)',\n",
              " 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days',\n",
              " '@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM',\n",
              " \"We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\",\n",
              " '@Impatientraider On second thought, there‚Äôs just not enough time for a DD :) But new shorts entering system. Sheep must be buying.',\n",
              " 'Jgh , but we have to go to Bayan :D bye',\n",
              " 'As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\\n\\nWell‚Ä¶ as the name implies :p.',\n",
              " '#FollowFriday @wncer1 @Defense_gouv for being top influencers in my community this week :)',\n",
              " \"Who Wouldn't Love These Big....Juicy....Selfies :) - http://t.co/QVzjgd1uFo http://t.co/oWBL11eQRY\",\n",
              " '@Mish23615351  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)',\n",
              " \"@jjulieredburn Perfect, so you already know what's waiting for you :)\",\n",
              " 'Great new opportunity for junior triathletes aged 12 and 13 at the Gatorade series! Get your entries in :) http://t.co/of3DyOzML0',\n",
              " 'Laying out a greetings card range for print today - love my job :-)',\n",
              " \"Friend's lunch... yummmm :)\\n#Nostalgia #TBS #KU.\",\n",
              " \"@RookieSenpai @arcadester it is the id conflict thanks for the help :D here's the screenshot of it working\",\n",
              " '@oohdawg_ Hi liv :))',\n",
              " 'Hello I need to know something can u fm me on Twitter?? ‚Äî sure thing :) dm me x http://t.co/W6Dy130BV7',\n",
              " '#FollowFriday @MBandScott_ @Eric_FLE @pointsolutions3 for being top new followers in my community this week :)',\n",
              " \"@rossbreadmore I've heard the Four Seasons is pretty dope. Penthouse, obvs #Gobigorgohome\\nHave fun y'all :)\",\n",
              " '@gculloty87 Yeah I suppose she was lol! Chat in a bit just off out x :))',\n",
              " 'Hello :) Get Youth Job Opportunities follow &gt;&gt; @tolajobjobs @maphisa301',\n",
              " \"üíÖüèΩüíã - :)))) haven't seen you in years\"]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V_Y5pHtc07i"
      },
      "source": [
        "**Given a list of tweets, preprocess each tweet from the list.**\n",
        "\n",
        "**Instructions**: Implement the *preprocess* function. You can do the text cleaning in any order you prefer.\n",
        "\n",
        "**Hint**: You may need to use regex expressions (use the resources provided above).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11qAmrMgS7QA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04cac1d7-13dd-45ba-af6f-cc44202b88b6"
      },
      "source": [
        "def preprocess(tweets):\n",
        "\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        tweets: a list of tweets\n",
        "    Output: \n",
        "        prepocessed_tweets: a list of preprocessed tweets\n",
        "    \"\"\"\n",
        "\n",
        "    ###you may need to create an additional list in which to store the processed tweets\n",
        "    prepocessed_tweets=[]\n",
        "    ###pay attention that some of the cleaning steps can be done at the document level, while others may be computed at word level\n",
        "\n",
        "\n",
        "    for tweet in tweets:\n",
        "    \n",
        "        ###remove new line characters '\\n'\n",
        "        preproc = re.sub('\\n', '', tweet)\n",
        "        ###remove links http://t.co/of3DyOzML0\n",
        "        # preproc = re.sub(\"http\\S+\",\" \",preproc)  #cleans only links\n",
        "        ###remove mentions '@'\n",
        "        preproc = re.sub('(@|https?)\\S+|#',\" \",preproc)  #cleans both links and mentions, also hashtags\n",
        "        ###remove hashtags '#'\n",
        "\n",
        "        ###lowercase text\n",
        "        preproc = preproc.lower()\n",
        "        ###remove emojis and emoticons 'üëå üç≠ :) :D'\n",
        "        preproc = emoji.get_emoji_regexp().sub(u'', preproc)\n",
        "        preproc = re.sub(emoticon_re, '', preproc)\n",
        "        ###remove digits\n",
        "        preproc = re.sub(' \\d+', '', preproc)\n",
        "        ###remove punctuation\n",
        "        preproc = re.sub(r'[^\\w\\s]','', preproc)\n",
        "        ###tokenize tweet into separate words\n",
        "        preproc = word_tokenize(preproc)\n",
        "        ###remove stopwords\n",
        "        preproc = [i for i in preproc if not i in stop_words_nltk]\n",
        "        ###lematization or stemming\n",
        "        preproc = [lemmatizer.lemmatize(j) for j in preproc]\n",
        "        prepocessed_tweets.append(preproc)\n",
        "    \n",
        "    return prepocessed_tweets\n",
        "\n",
        "preprocess(tweets)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['followfriday', 'top', 'engaged', 'member', 'community', 'week'],\n",
              " ['hey',\n",
              "  'james',\n",
              "  'odd',\n",
              "  'please',\n",
              "  'call',\n",
              "  'contact',\n",
              "  'centre',\n",
              "  'able',\n",
              "  'assist',\n",
              "  'many',\n",
              "  'thanks'],\n",
              " ['listen', 'last', 'night', 'bleed', 'amazing', 'track', 'scotland'],\n",
              " ['congrats'],\n",
              " ['yeaaaah',\n",
              "  'yippppy',\n",
              "  'accnt',\n",
              "  'verified',\n",
              "  'rqst',\n",
              "  'succeed',\n",
              "  'got',\n",
              "  'blue',\n",
              "  'tick',\n",
              "  'mark',\n",
              "  'fb',\n",
              "  'profile',\n",
              "  'day'],\n",
              " ['one', 'irresistible', 'flipkartfashionfriday'],\n",
              " ['dont',\n",
              "  'like',\n",
              "  'keep',\n",
              "  'lovely',\n",
              "  'customer',\n",
              "  'waiting',\n",
              "  'long',\n",
              "  'hope',\n",
              "  'enjoy',\n",
              "  'happy',\n",
              "  'friday',\n",
              "  'lwwf'],\n",
              " ['second',\n",
              "  'thought',\n",
              "  'there',\n",
              "  'enough',\n",
              "  'time',\n",
              "  'dd',\n",
              "  'new',\n",
              "  'short',\n",
              "  'entering',\n",
              "  'system',\n",
              "  'sheep',\n",
              "  'must',\n",
              "  'buying'],\n",
              " ['jgh', 'go', 'bayan', 'bye'],\n",
              " ['act',\n",
              "  'mischievousness',\n",
              "  'calling',\n",
              "  'etl',\n",
              "  'layer',\n",
              "  'inhouse',\n",
              "  'warehousing',\n",
              "  'app',\n",
              "  'katamariwell',\n",
              "  'name',\n",
              "  'implies'],\n",
              " ['followfriday', 'top', 'influencers', 'community', 'week'],\n",
              " ['wouldnt', 'love', 'bigjuicyselfies'],\n",
              " ['follow', 'follow', 'u', 'back'],\n",
              " ['perfect', 'already', 'know', 'whats', 'waiting'],\n",
              " ['great',\n",
              "  'new',\n",
              "  'opportunity',\n",
              "  'junior',\n",
              "  'triathletes',\n",
              "  'aged',\n",
              "  'gatorade',\n",
              "  'series',\n",
              "  'get',\n",
              "  'entry'],\n",
              " ['laying', 'greeting', 'card', 'range', 'print', 'today', 'love', 'job'],\n",
              " ['friend', 'lunch', 'yummmm', 'nostalgia', 'tb', 'ku'],\n",
              " ['id', 'conflict', 'thanks', 'help', 'here', 'screenshot', 'working'],\n",
              " ['hi', 'liv'],\n",
              " ['hello',\n",
              "  'need',\n",
              "  'know',\n",
              "  'something',\n",
              "  'u',\n",
              "  'fm',\n",
              "  'twitter',\n",
              "  'sure',\n",
              "  'thing',\n",
              "  'dm',\n",
              "  'x'],\n",
              " ['followfriday', 'top', 'new', 'follower', 'community', 'week'],\n",
              " ['ive',\n",
              "  'heard',\n",
              "  'four',\n",
              "  'season',\n",
              "  'pretty',\n",
              "  'dope',\n",
              "  'penthouse',\n",
              "  'obvs',\n",
              "  'gobigorgohomehave',\n",
              "  'fun',\n",
              "  'yall'],\n",
              " ['yeah', 'suppose', 'lol', 'chat', 'bit', 'x'],\n",
              " ['hello', 'get', 'youth', 'job', 'opportunity', 'follow', 'gtgt'],\n",
              " ['havent', 'seen', 'year']]"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw7l8oeP9Udo"
      },
      "source": [
        "### Don't mind me, just verifying some stuff\n",
        "\n",
        "# procc_tweet=[]\n",
        "# for t in tweets:\n",
        "#   # procc_tweet.append(re.sub('\\n', '', t))\n",
        "#   # procc_tweet.append(t.lower())\n",
        "#   procc_tweet.append(tokenized_text_without_stopwords = [i for i in tokenized_text_nltk if not i in stop_words_nltk])\n"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvZEUG_P988N"
      },
      "source": [
        "# procc_tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNPLCHObe0O5"
      },
      "source": [
        "Tools:\n",
        "\n",
        "* [Preprocessing library for Twitter](https://github.com/s/preprocessor)\n",
        "* [Emoji library](https://github.com/carpedm20/emoji)\n",
        "* [Demoji library](https://github.com/bsolomon1124/demoji)\n",
        "* [Gensim](https://radimrehurek.com/gensim/)\n",
        "\n",
        "\n",
        "Further reading:\n",
        "\n",
        "* [Lexical Normalization](https://arxiv.org/pdf/1710.03476.pdf)\n",
        "* [On learning and representing social meaning in NLP: a sociolinguistic perspective](https://aclanthology.org/2021.naacl-main.50.pdf)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}